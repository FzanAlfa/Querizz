{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "import random\n",
        "import json"
      ],
      "metadata": {
        "id": "j_F1dhOTxZql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple Choice Question Model"
      ],
      "metadata": {
        "id": "Kudk_mUf00So"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Data\n",
        "data = pd.DataFrame({\n",
        "    'paragraph': [\n",
        "        'This is an example paragraph for an LSTM model.',\n",
        "        'Another example of a paragraph for the model to learn.',\n",
        "        'LSTM models are used for sequence prediction.',\n",
        "        'Neural networks can learn complex patterns.',\n",
        "        'Text classification can be done using LSTM.'\n",
        "    ],\n",
        "    'question': [\n",
        "        'What is this paragraph for?',\n",
        "        'What is this an example of?',\n",
        "        'What are LSTM models used for?',\n",
        "        'What can neural networks learn?',\n",
        "        'What can be done using LSTM?'\n",
        "    ],\n",
        "    'answer': [\n",
        "        'LSTM model',\n",
        "        'Model to learn',\n",
        "        'Sequence prediction',\n",
        "        'Complex patterns',\n",
        "        'Text classification'\n",
        "    ],\n",
        "    'distractor1': [\n",
        "        'Decision Tree',\n",
        "        'Random Forest',\n",
        "        'Support Vector Machine',\n",
        "        'Linear Regression',\n",
        "        'Naive Bayes'\n",
        "    ],\n",
        "    'distractor2': [\n",
        "        'Random Forest',\n",
        "        'Support Vector Machine',\n",
        "        'Naive Bayes',\n",
        "        'Logistic Regression',\n",
        "        'Decision Tree'\n",
        "    ],\n",
        "    'distractor3': [\n",
        "        'Support Vector Machine',\n",
        "        'Naive Bayes',\n",
        "        'Decision Tree',\n",
        "        'Random Forest',\n",
        "        'Logistic Regression'\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer()\n",
        "# Train the tokenizer on texts\n",
        "tokenizer.fit_on_texts(data['paragraph'])\n",
        "tokenizer.fit_on_texts(data['question'])\n",
        "tokenizer.fit_on_texts(data['answer'])\n",
        "tokenizer.fit_on_texts(data['distractor1'])\n",
        "tokenizer.fit_on_texts(data['distractor2'])\n",
        "tokenizer.fit_on_texts(data['distractor3'])\n",
        "# Get the vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Convert text to sequences\n",
        "paragraph_sequences = tokenizer.texts_to_sequences(data['paragraph'])\n",
        "max_sequence_length = max(len(seq) for seq in paragraph_sequences)\n",
        "# Pad the paragraph sequences so that they all have the same length\n",
        "padded_paragraphs = pad_sequences(paragraph_sequences, padding='post', maxlen=max_sequence_length)\n",
        "\n",
        "# Convert targets to sequences\n",
        "target_sequences = {}\n",
        "for target in ['question', 'answer', 'distractor1', 'distractor2', 'distractor3']:\n",
        "    target_data = tokenizer.texts_to_sequences(data[target])\n",
        "    target_sequences[target] = pad_sequences(target_data, padding='post', maxlen=max_sequence_length)\n",
        "\n",
        "# Verify that the target arrays contain the same number of samples as the padded paragraphs\n",
        "for target in target_sequences:\n",
        "    assert padded_paragraphs.shape[0] == target_sequences[target].shape[0], f\"Number of samples in padded_paragraphs and target_{target} do not match.\"\n",
        "\n",
        "# Verify that the target sequences are correctly flattened\n",
        "for target in target_sequences:\n",
        "    target_sequences[target] = np.expand_dims(np.argmax(target_sequences[target], axis=1), -1)\n",
        "\n",
        "target_dict = {target + '_output': target_sequences[target] for target in target_sequences}"
      ],
      "metadata": {
        "id": "4hCBR-GoyaBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model architecture\n",
        "input_shape = (max_sequence_length,)\n",
        "input_layer = Input(shape=input_shape)\n",
        "embedding_layer = Embedding(vocab_size, 128)(input_layer)\n",
        "lstm_layer = LSTM(256, return_sequences=False)(embedding_layer)\n",
        "\n",
        "# Output layers for each task\n",
        "outputs = {}\n",
        "# Iterate over each target to create a corresponding output layer\n",
        "for target in ['question', 'answer', 'distractor1', 'distractor2', 'distractor3']:\n",
        "    outputs[target + '_output'] = Dense(vocab_size, activation='softmax', name=target + '_output')(lstm_layer)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=input_layer, outputs=outputs)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print shapes for debugging\n",
        "print(\"Shape of padded_paragraphs:\", padded_paragraphs.shape)\n",
        "for target in target_sequences:\n",
        "    print(f\"Shape of {target}:\", target_sequences[target].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83nQ1SBG1IKH",
        "outputId": "522586e6-751d-4b7a-f3f0-873651499d1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of padded_paragraphs: (5, 10)\n",
            "Shape of question: (5, 1)\n",
            "Shape of answer: (5, 1)\n",
            "Shape of distractor1: (5, 1)\n",
            "Shape of distractor2: (5, 1)\n",
            "Shape of distractor3: (5, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model using the padded_paragraphs as input and target_dict as outputs\n",
        "NUM_EPOCHS = 10\n",
        "model.fit(padded_paragraphs, target_dict, epochs=NUM_EPOCHS, batch_size=10)\n",
        "\n",
        "\n",
        "def sample_from_predictions(predictions, temperature=1.0):\n",
        "    predictions = np.asarray(predictions).astype('float64')\n",
        "    predictions = np.log(predictions) / temperature\n",
        "    exp_predictions = np.exp(predictions)\n",
        "    predictions = exp_predictions / np.sum(exp_predictions)\n",
        "    probabilities = np.random.multinomial(1, predictions, 1)\n",
        "    return np.argmax(probabilities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "andDZY6b1K6a",
        "outputId": "af7cbc7c-03c1-4891-96b8-fadd8fc96cdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 3s 3s/step - loss: 18.8001 - answer_output_loss: 3.7626 - distractor1_output_loss: 3.7565 - distractor2_output_loss: 3.7593 - distractor3_output_loss: 3.7590 - question_output_loss: 3.7627 - answer_output_accuracy: 0.0000e+00 - distractor1_output_accuracy: 0.0000e+00 - distractor2_output_accuracy: 0.0000e+00 - distractor3_output_accuracy: 0.0000e+00 - question_output_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 18.6835 - answer_output_loss: 3.7286 - distractor1_output_loss: 3.7375 - distractor2_output_loss: 3.7360 - distractor3_output_loss: 3.7386 - question_output_loss: 3.7429 - answer_output_accuracy: 1.0000 - distractor1_output_accuracy: 0.4000 - distractor2_output_accuracy: 0.6000 - distractor3_output_accuracy: 0.8000 - question_output_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 18.5555 - answer_output_loss: 3.6907 - distractor1_output_loss: 3.7168 - distractor2_output_loss: 3.7100 - distractor3_output_loss: 3.7163 - question_output_loss: 3.7216 - answer_output_accuracy: 1.0000 - distractor1_output_accuracy: 0.8000 - distractor2_output_accuracy: 0.6000 - distractor3_output_accuracy: 0.6000 - question_output_accuracy: 0.6000\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 18.4008 - answer_output_loss: 3.6444 - distractor1_output_loss: 3.6920 - distractor2_output_loss: 3.6790 - distractor3_output_loss: 3.6889 - question_output_loss: 3.6966 - answer_output_accuracy: 1.0000 - distractor1_output_accuracy: 0.8000 - distractor2_output_accuracy: 0.6000 - distractor3_output_accuracy: 0.6000 - question_output_accuracy: 0.6000\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 18.1997 - answer_output_loss: 3.5833 - distractor1_output_loss: 3.6597 - distractor2_output_loss: 3.6392 - distractor3_output_loss: 3.6528 - question_output_loss: 3.6648 - answer_output_accuracy: 1.0000 - distractor1_output_accuracy: 0.8000 - distractor2_output_accuracy: 0.6000 - distractor3_output_accuracy: 0.6000 - question_output_accuracy: 0.4000\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 17.9232 - answer_output_loss: 3.4987 - distractor1_output_loss: 3.6151 - distractor2_output_loss: 3.5851 - distractor3_output_loss: 3.6026 - question_output_loss: 3.6217 - answer_output_accuracy: 1.0000 - distractor1_output_accuracy: 0.6000 - distractor2_output_accuracy: 0.6000 - distractor3_output_accuracy: 0.6000 - question_output_accuracy: 0.4000\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 17.5256 - answer_output_loss: 3.3765 - distractor1_output_loss: 3.5506 - distractor2_output_loss: 3.5080 - distractor3_output_loss: 3.5301 - question_output_loss: 3.5604 - answer_output_accuracy: 1.0000 - distractor1_output_accuracy: 0.6000 - distractor2_output_accuracy: 0.6000 - distractor3_output_accuracy: 0.6000 - question_output_accuracy: 0.4000\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 16.9337 - answer_output_loss: 3.1947 - distractor1_output_loss: 3.4539 - distractor2_output_loss: 3.3939 - distractor3_output_loss: 3.4219 - question_output_loss: 3.4693 - answer_output_accuracy: 1.0000 - distractor1_output_accuracy: 0.6000 - distractor2_output_accuracy: 0.6000 - distractor3_output_accuracy: 0.6000 - question_output_accuracy: 0.4000\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 16.0270 - answer_output_loss: 2.9170 - distractor1_output_loss: 3.3046 - distractor2_output_loss: 3.2197 - distractor3_output_loss: 3.2563 - question_output_loss: 3.3294 - answer_output_accuracy: 1.0000 - distractor1_output_accuracy: 0.6000 - distractor2_output_accuracy: 0.6000 - distractor3_output_accuracy: 0.6000 - question_output_accuracy: 0.4000\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 14.6092 - answer_output_loss: 2.4856 - distractor1_output_loss: 3.0692 - distractor2_output_loss: 2.9473 - distractor3_output_loss: 2.9974 - question_output_loss: 3.1096 - answer_output_accuracy: 1.0000 - distractor1_output_accuracy: 0.6000 - distractor2_output_accuracy: 0.6000 - distractor3_output_accuracy: 0.6000 - question_output_accuracy: 0.4000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_from_paragraph(model, paragraph, tokenizer, max_length=20, temperature=1.0):\n",
        "    sequence = tokenizer.texts_to_sequences([paragraph])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=padded_paragraphs.shape[1], padding='post')\n",
        "    predictions = model.predict(padded_sequence)\n",
        "\n",
        "    generated_text = []\n",
        "     # Generate text based on predictions for a maximum of max_length words\n",
        "    for _ in range(max_length):\n",
        "        question_prediction = sample_from_predictions(predictions['question_output'][0], temperature)\n",
        "        word = tokenizer.index_word.get(question_prediction, '<UNK>')\n",
        "        if word == '<UNK>':\n",
        "            break\n",
        "        generated_text.append(word)\n",
        "        padded_sequence = pad_sequences([tokenizer.texts_to_sequences([word])[0]], maxlen=padded_paragraphs.shape[1], padding='post')\n",
        "        predictions = model.predict(padded_sequence)\n",
        "\n",
        "    return ' '.join(generated_text)\n",
        "\n",
        "# Set new paragraph\n",
        "new_paragraph = \"\"\n",
        "# Generate predicted question, answer, and distractors for the new paragraph\n",
        "predicted_question = predict_from_paragraph(model, new_paragraph, tokenizer, max_length=10, temperature=0.7)\n",
        "predicted_answer = predict_from_paragraph(model, new_paragraph, tokenizer, max_length=10, temperature=0.7)\n",
        "predicted_distractor1 = predict_from_paragraph(model, new_paragraph, tokenizer, max_length=10, temperature=0.7)\n",
        "predicted_distractor2 = predict_from_paragraph(model, new_paragraph, tokenizer, max_length=10, temperature=0.7)\n",
        "predicted_distractor3 = predict_from_paragraph(model, new_paragraph, tokenizer, max_length=10, temperature=0.7)\n",
        "\n",
        "print(f'Pertanyaan: {predicted_question}')\n",
        "print(f'Jawaban: {predicted_answer}')\n",
        "print(f'Distraktor 1: {predicted_distractor1}')\n",
        "print(f'Distraktor 2: {predicted_distractor2}')\n",
        "print(f'Distraktor 3: {predicted_distractor3}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMTXx3UH1Qde",
        "outputId": "5d562bd8-8d93-47cf-b162-a75f8742daeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 364ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Pertanyaan: model using prediction what model what a linear model model\n",
            "Jawaban: model what networks vector paragraph what model model be using\n",
            "Distraktor 1: model of another decision model model learn model of machine\n",
            "Distraktor 2: what learn model what this model a is model complex\n",
            "Distraktor 3: what model model model what what model can model a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Model"
      ],
      "metadata": {
        "id": "InfUB0Xc1hYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model in format h5\n",
        "model.save('my_model.h5')\n",
        "# Load the saved model from file\n",
        "loaded_model = load_model('my_model.h5')\n",
        "print(\"Model loaded successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5p2BC90_1RLy",
        "outputId": "fe6e301e-418a-4579-ba1f-27b1b1327adb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "loaded_model = load_model('my_model.h5')\n",
        "\n",
        "# Convert the model architecture to JSON format\n",
        "model_json = loaded_model.to_json()\n",
        "\n",
        "with open(\"model_architecture.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "files.download('model_architecture.json')"
      ],
      "metadata": {
        "id": "pPNnxqSE1UBx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}